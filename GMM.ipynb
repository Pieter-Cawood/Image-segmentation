{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import json\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from cachier import cachier\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from skimage import img_as_float32\n",
    "import cv2 as cv2\n",
    "from math import pi, e, sqrt, cos, sin\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.spatial.distance import euclidean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load puzzle images, masks & corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm = partial(tqdm, position=0, leave=True)\n",
    "cachier = partial(cachier, pickle_reload=False, cache_dir='data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████████████████████████████████████████████████████████████| 48/48 [00:01<00:00, 35.27it/s]\n",
      "Loading Masks: 100%|██████████████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 212.39it/s]\n"
     ]
    }
   ],
   "source": [
    "SIZE = (768, 1024)\n",
    "\n",
    "DATA_PATH_PAIRS = list(zip(\n",
    "    natsorted(glob(f'./data/images-{SIZE[1]}x{SIZE[0]}/*.png')),\n",
    "    natsorted(glob(f'./data/masks-{SIZE[1]}x{SIZE[0]}/*.png')),\n",
    "))\n",
    "DATA_IMGS = np.array(\n",
    "    [img_as_float32(imageio.imread(img_path)) for img_path, _ in tqdm(DATA_PATH_PAIRS, 'Loading Images')])\n",
    "DATA_MSKS = np.array(\n",
    "    [img_as_float32(imageio.imread(msk_path)) for _, msk_path in tqdm(DATA_PATH_PAIRS, 'Loading Masks')])\n",
    "\n",
    "assert DATA_IMGS.shape == (48, SIZE[0], SIZE[1], 3)\n",
    "assert DATA_MSKS.shape == (48, SIZE[0], SIZE[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./data/corners.json', mode='r') as f:\n",
    "    DATA_CORNER_NAMES, DATA_CORNERS = json.load(f)\n",
    "    DATA_CORNERS = np.array(DATA_CORNERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data):\n",
    "    return np.subtract(data, np.min(data))/ np.subtract(np.max(data), np.min(data))\n",
    "\n",
    "def show_image(image,cs=False,cmap= None, title=None):\n",
    "    if cs:\n",
    "        norm_img = normalise_data(image)\n",
    "    else:\n",
    "        norm_img = image\n",
    "    plt.title(title)\n",
    "    plt.imshow(norm_img, cmap= cmap)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataUtils(object):\n",
    "    \n",
    "    def train_test_split(self, images, masks, train_size=0.7, validation_size=0.15, test_size=0.15):\n",
    "        all_indices = np.arange(len(images))\n",
    "        np.random.shuffle(all_indices)\n",
    "        train_indices = all_indices[0: int(len(images)* train_size) + 1]\n",
    "        validate_inidices = all_indices[len(train_indices): len(train_indices) + int(len(images)* validation_size)]\n",
    "        test_indices = all_indices[len(train_indices)+len(validate_inidices): len(train_indices)+len(validate_inidices)+ int(len(images)* test_size)]\n",
    "\n",
    "        train_images = np.array([images[i] for i in train_indices])\n",
    "        train_masks = np.array([masks[i] for i in train_indices])\n",
    "        \n",
    "        validate_images = np.array([images[i] for i in validate_inidices])\n",
    "        validate_masks = np.array([masks[i] for i in validate_inidices])\n",
    "        \n",
    "        test_images = np.array([images[i] for i in test_indices])\n",
    "        test_masks = np.array([masks[i] for i in test_indices])\n",
    "        return train_images, train_masks, validate_images, validate_masks, test_images, test_masks\n",
    "\n",
    "\n",
    "    def _get_kernel(self, g, sigma, size, k=None):   \n",
    "        \"\"\"\n",
    "        Function to map kernel function to numpy matrix, \n",
    "        Note result is normalized.\n",
    "        \n",
    "        \"\"\"\n",
    "        # Center x and y coordinates and return a numpy matrix from g function\n",
    "        kernel =  np.fromfunction(lambda x, y: \n",
    "                                 g((x - (size - 1) / 2), (y - (size - 1) / 2), sigma, k),\n",
    "                                 (size, size))  \n",
    "        return kernel.astype(np.float32)\n",
    "\n",
    "    def _dog(self, sigma, size, k):\n",
    "        \"\"\"\n",
    "        Get a Difference of Gaussian filter\n",
    "    \n",
    "        \"\"\"\n",
    "        def _g(x, y, sigma, k):\n",
    "            return (2*pi*sigma**2) * \\\n",
    "                    e**(-1 * ((x**2 + y**2) / (2 * sigma**2))) - \\\n",
    "                    (1 / 2 * pi * k**2 * sigma**2) *\\\n",
    "                    e**(-1 * ((x**2 + y**2)/(2 * k**2 * sigma**2)))\n",
    "                \n",
    "        if k <= 1.0:\n",
    "            raise ValueError(\"k must be > 1.0\")\n",
    "        \n",
    "        return self._get_kernel(_g, sigma, size, k)\n",
    "    \n",
    "    \n",
    "    def extract(self, image, gaus=(3,3), dog_sigma=10, dog_size=49, dog_k=5):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        image : numpy array of shape (n_sample, col, row, n_channels)\n",
    "        gaus : tuple of gaussian kernel for blurring input image\n",
    "        dog_sigma : int, sigma value for DoG features\n",
    "        dog_size  : int, size of DoG kernel\n",
    "        dog_k : int, k parameter for DoG \n",
    "        show_feature_plots : bool, verbose option to plot features\n",
    "        \n",
    "        Returns\n",
    "        ---------- \n",
    "        numpy array : 9 x flattened image shape (786432) features of image.\n",
    "        1 - 3: R, G, B channels\n",
    "        4 - 6: DoG of RGB channels\n",
    "        7 - 9: H, S, V channels\n",
    "    \n",
    "        \"\"\"\n",
    "        # Gaussian blur image as a first step to reduce noise\n",
    "        image_ = cv2.GaussianBlur(image, gaus, 0).astype(np.float32)  \n",
    "        # RBG Features\n",
    "        features = None\n",
    "        for rgb_channel in cv2.split(image_):\n",
    "            if features is None:\n",
    "                features = np.array(normalise_data(rgb_channel.flatten().astype(np.float32)))\n",
    "            else:\n",
    "                features = np.vstack((features, normalise_data(rgb_channel.flatten().astype(np.float32))))\n",
    "                \n",
    "        # DoG features\n",
    "        dog_kernel = self._dog(dog_sigma, dog_size, dog_k)\n",
    "        convolved = cv2.filter2D(image_, -1, dog_kernel)              \n",
    "        for convolved_rgb_channel in cv2.split(convolved):\n",
    "            features = np.vstack((features, normalise_data(convolved_rgb_channel.flatten().astype(np.float32))))\n",
    "\n",
    "        # HSV features\n",
    "        hsv_image = cv2.cvtColor(image_, cv2.COLOR_RGB2HSV)\n",
    "        for hsv_channel in cv2.split(hsv_image):\n",
    "            features = np.vstack((features, normalise_data(hsv_channel.flatten().astype(np.float32))))\n",
    "  \n",
    "        return features\n",
    "    \n",
    "    def apply_mask(self, features, mask, invert_mask=False):\n",
    "        \"\"\"Apply the mask to a data set\"\"\"\n",
    "        data = []\n",
    "        mask_ = mask.astype(np.bool).flatten()\n",
    "        if invert_mask:\n",
    "            mask_ = np.invert(mask_)\n",
    "        for feature in features:\n",
    "            masked_feature = feature[mask_ == True]\n",
    "            data.append(masked_feature.flatten())\n",
    "        return np.array(data)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils = ImageDataUtils()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation and training split\n",
    "Split data raondmly into sets of 70% training, 15% validation and 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_masks, \\\n",
    "validate_images, validate_masks, \\\n",
    "test_images, test_masks = data_utils.train_test_split(DATA_IMGS, DATA_MSKS, \n",
    "                                                      train_size=0.7, \n",
    "                                                      validation_size=0.15, \n",
    "                                                      test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data, and extract the features:\n",
    "Flattened and normalised feature vectors for each image of\n",
    "- RGB channels\n",
    "- DoG on RGB channels\n",
    "- HSV channels\n",
    "\n",
    "The training data is split into two sets (foreground and background) to train two GMM models. Lastly all of the training\n",
    "data are concatenated into a signle dimension so that we have data sets in the format (n_features, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([data_utils.extract(train_image) for train_image in train_images])\n",
    "validate_data = np.array([data_utils.extract(validate_image) for validate_image in validate_images])\n",
    "test_data = np.array([data_utils.extract(test_image) for test_image in test_images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use masks to split training data to foreground and background data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bg_data = [data_utils.apply_mask(features, mask, invert_mask=True) \n",
    "                for features, mask in zip(train_data,train_masks)]\n",
    "train_fg_data = [data_utils.apply_mask(features, mask, invert_mask=False) \n",
    "                for features, mask in zip(train_data,train_masks)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bg_data = np.concatenate(train_bg_data, axis=1)\n",
    "train_fg_data = np.concatenate(train_fg_data, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background of first piece**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model (GMM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our data is composed of intensities that might be modelled using gaussian distributions, we investigate the use of a Mixture Gausssian Model (GMM), which is an unsupervised learning approach that aims to fit a mixture of gasussian distributions to model the data with $d$ dimensions of features. And we want to use the GMM to fit clusters or mixture of gaussians to our data set, but we do not know where they are, or how they are shaped.\n",
    "It differs from normal KNN, since the clusters are gaussian distributions, which are parameterised by a mean and covariance. Which allows us to model the probability that our data belongs to either clusters in elliptical shapes, as opposed to the circular shapes clustered by KNN.\n",
    "\n",
    "![GMM](resources/GMM_illistration.png)\n",
    "\n",
    "### Expectation Maximisation (EM)\n",
    "We find the gaussian mixture model by using the EM algorithm to find k to minimize $\\frac{(x-u_k)^2}{\\sigma^2}$.\n",
    "EM introduces a latent variable $h$ for each cluster $K$, and the GMM is a marginalisation of the joint probability distribution $P(\\vec{x}, h)$, where $\\vec{x}$ represents our feature vectors.  We define a variable $h \\in \\{1 \\dots K\\}$ and then the probability distribution for each categorical variable is written as <br>\n",
    "$Pr(\\times|h, \\theta) = Norm_x[\\mu_h, \\Sigma_h]$ <br>\n",
    "$Pr(h|\\theta) = Cat_h[\\lambda]$ <br>\n",
    "\n",
    "Then we can recover the density by marginalising out the $h$ from the joint probability distribution $Pr(\\vec{x},h)$ <br>\n",
    "$\n",
    "\\begin{align}\n",
    "Pr(\\vec{x}|\\theta) & = \\sum^K_{k=1} Pr(\\vec{x}, h = k | \\theta)\\\\\n",
    "                   & = \\sum^K_{k=1} Pr(\\vec{x}, h = k, \\theta) Pr(h=k|\\theta)\\\\\n",
    "                   & = \\sum^K_{k=1} \\lambda_k Norm_\\vec{x}[\\mu_k, \\Sigma_h]   \\\\\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "![GMM](resources/GMM_marginilisation.png)\n",
    "\n",
    "This formulation allows us to fit the model with the Expectation Maximisation algorithm using closed-form solutions in an iterative process. The goal is to learn the model parameters $\\theta = \\{\\lambda_{1 \\dots K}, \\mu_{1 \\dots K}, \\Sigma_{1 \\dots K} \\}$ from our training features $\\vec{x}$. The two steps of the EM algorithm include\n",
    "\n",
    "#### 1. E-Step (Expectation)\n",
    "The probability of $h$ is computed using the current estimates of the parameters $\\theta$. So for each point we estimate the probability that each Gaussian generated it $Pr(h_i = k | \\vec{x}_i, \\theta^{[t]})$, which might also be described as the responsibility of the $k^{th}$ Gaussian for the $i^{th}$ data point expressed by <br>\n",
    "$r_{ik} = \\frac{\\lambda_k Norm_{x_i} [\\mu_k, \\Sigma_k]}{\\sum^K_{j=1}\\lambda_j Norm_\\vec{x_i} [\\mu_j, \\Sigma_j]}$\n",
    "\n",
    "![GMM](resources/GMM_Estep.png)\n",
    "\n",
    "\n",
    "#### 1. M-Step (Maximisation)\n",
    "Using the computed resposibilities the maximisation step then updates the model parameters $\\theta$. And in our case we use maximum likelihood estimation that allows the following closed form update rules <br>\n",
    "\n",
    "\n",
    "$\\lambda_k^{[t + 1]} = \\frac{\\sum^I_{i=1} r_{ik}} {\\sum^K_{j=1}\\sum^I_{i=1} r_{ij}  }   $\n",
    "\n",
    "$\\mu_k^{[t + 1]} = \\frac{\\sum^I_{i=1} r_{ik} \\vec{x_i}} {\\sum^I_{i=1} r_{ik}} $\n",
    "\n",
    "$\\Sigma_k^{[t + 1]} = \\frac{\\sum^I_{i=1} r_{ik} (\\vec{x_i} - \\mu_k^{[t + 1]}) (\\vec{x_i} - \\mu_k^{[t + 1]})^T}{\\sum^I_{i=1} r_{ik}}$\n",
    "\n",
    "![GMM](resources/GMM_Mstep.png)\n",
    "\n",
    "#### The EM-flow\n",
    "![EM](resources/EM_flow.png)\n",
    "- The $k$ clusters center are randomly initialised \n",
    "- The responsibilities are recomputed during the E-Step, and the model parameters are updated during the M-Step.\n",
    "- The iterative process is repeated until convergence (Parameter change is smaller then some threshold.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GMM using EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GuassianParameters(object):\n",
    "    \"\"\"Distribution parameters\"\"\"\n",
    "\n",
    "    def __init__(self, lmbda, mu, cov):\n",
    "        self.lmbda = lmbda\n",
    "        self.mu = mu\n",
    "        self.cov = cov\n",
    "\n",
    "    def difference_between(self, other):\n",
    "        return abs(self.lmbda - other.lmbda) + \\\n",
    "               euclidean(self.mu, other.mu) + \\\n",
    "               euclidean(self.cov.flatten(), other.cov.flatten())\n",
    "\n",
    "\n",
    "class GaussianMixtureModel(object):\n",
    "    \"\"\"Multivariate Gaussian Mixture Model\"\"\"\n",
    "\n",
    "    def _get_init_params(self, X):\n",
    "        n_features = X.shape[1]\n",
    "        # Initial weigths evenly distributed\n",
    "        lmbda = 1 / self.k_size\n",
    "        # Initial means, random observations from input data\n",
    "        mu = np.random.choice(X.flatten(), n_features)\n",
    "        # Iniital covariance\n",
    "        cov = np.random.random((n_features, n_features))\n",
    "        cov *= cov.T\n",
    "        cov += n_features * np.eye(n_features)\n",
    "        return GuassianParameters(lmbda, mu, cov)\n",
    "\n",
    "    def __init__(self, k_size=3, tol=1e-2, max_iter=100, verbose=True):\n",
    "        self.fitted = False\n",
    "        self.k_size = k_size\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "        self.verbose = verbose\n",
    "        self.fitted_parameters = None\n",
    "\n",
    "    def _estep(self, X, gaussian_parameters):\n",
    "        \"\"\"Calculate the responsibility for each cluster for all of the data points,\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        responsibilities = np.zeros((n_samples, self.k_size))\n",
    "        weighted_gaussians = np.zeros((n_samples, self.k_size))\n",
    "        # Get normal distributions using current parameter set\n",
    "        for k in range(self.k_size):\n",
    "            weighted_gaussians[:, k] = gaussian_parameters[k].lmbda * \\\n",
    "                                       multivariate_normal.pdf(X, gaussian_parameters[k].mu, gaussian_parameters[k].cov)\n",
    "        # Compute responsibility for each cluster\n",
    "        for k in range(self.k_size):\n",
    "            responsibilities[:, k] = weighted_gaussians[:, k] / np.sum(weighted_gaussians, axis=1, initial=1e-10)\n",
    "        return responsibilities\n",
    "\n",
    "    def _mstep(self, X, responsibilities):\n",
    "        \"\"\"Compute model parameters using maximum likelikhood closed form solutions\"\"\"\n",
    "        gaussian_parameters = []\n",
    "        for k in range(self.k_size):\n",
    "            k_sum_responsibility = np.sum(responsibilities[:, k])\n",
    "            lmbda = k_sum_responsibility / (np.sum(np.sum(responsibilities, axis=1)))\n",
    "            mu = np.dot(responsibilities[:, k], X) / k_sum_responsibility\n",
    "            standardised_x = X - mu\n",
    "            cov = np.dot((standardised_x.T * responsibilities[:, k]), standardised_x) / k_sum_responsibility\n",
    "            # Add small value to diagonal to prevent singular matrix\n",
    "            cov += np.eye(len(cov)) * 1e-10\n",
    "            gaussian_parameters.append(GuassianParameters(lmbda, mu, cov))\n",
    "        return gaussian_parameters\n",
    "    \n",
    "    def save_parameters(self, filename):\n",
    "        \"\"\"Save the model parameters to file\"\"\"\n",
    "        assert self.fitted, \"Model is not yet fitted.\"\n",
    "        np.save(filename, self.fitted_parameters)\n",
    "    \n",
    "    def load_parameters(self, filename):\n",
    "        \"\"\"Load the model parameters from file\"\"\"\n",
    "        self.fitted_parameters = np.load(filename +'.npy',allow_pickle=True)\n",
    "        self.fitted = True\n",
    "        \n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model according to the given training data.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, of shape (n_samples, n_features)\n",
    "            Training vector, where n_samples is the number of samples/pixels and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        \"\"\"\n",
    "        converged = False\n",
    "        run_iteration = 1\n",
    "        # Initialise parameters\n",
    "        gaussian_parameters = [self._get_init_params(X) for _k in range(self.k_size)]\n",
    "        while not converged and run_iteration <= self.max_iter:\n",
    "            # E-step\n",
    "            responsibilities = self._estep(X, gaussian_parameters)\n",
    "            # M-step\n",
    "            new_gaussian_parameters = self._mstep(X, responsibilities)\n",
    "            # Iterate until parameters experience change smaller than tolerance\n",
    "            total_change = 0\n",
    "            for k, parameters in enumerate(new_gaussian_parameters):\n",
    "                total_change += parameters.difference_between(gaussian_parameters[k])\n",
    "            if total_change < self.tol:\n",
    "                converged = True\n",
    "            gaussian_parameters = new_gaussian_parameters\n",
    "            if self.verbose:\n",
    "                print(\"Iteration #{0}, change in parameters {1}\".format(run_iteration, total_change))\n",
    "            run_iteration += 1\n",
    "            \n",
    "        self.fitted_parameters = gaussian_parameters\n",
    "        self.fitted = True\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(\"Finished in {0} iterations\".format(run_iteration - 1))\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict's the probability density the input observations by marginalising\n",
    "        out the joint probability distributions.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, of shape (n_samples, n_features)\n",
    "            Testing vector, where n_samples is the number of samples/pixels and\n",
    "            n_features is the number of features.\n",
    "        \n",
    "        Returns\n",
    "        ----------\n",
    "        y : numpy array, of shape (n_samples)\n",
    "            Estimated probability density from the mixture of gaussian models.\n",
    "        \"\"\"\n",
    "        assert self.fitted, \"Model is not yet fitted.\"\n",
    "        n_samples = X.shape[0]\n",
    "        gaussian_pdfs = np.zeros((n_samples, self.k_size))\n",
    "        for k in range(self.k_size):\n",
    "            gaussian_pdfs[:, k] = self.fitted_parameters[k].lmbda *\\\n",
    "            multivariate_normal.pdf(X, self.fitted_parameters[k].mu, self.fitted_parameters[k].cov)\n",
    "        # Return marginalised distribution\n",
    "        return np.sum(gaussian_pdfs, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1, change in parameters 86.6708328818466\n",
      "Iteration #2, change in parameters 0.021491904291868695\n",
      "Iteration #3, change in parameters 0.060419099836768325\n",
      "Iteration #4, change in parameters 0.16156940590998498\n",
      "Iteration #5, change in parameters 0.20987194105482554\n",
      "Iteration #6, change in parameters 0.19784695424288581\n",
      "Iteration #7, change in parameters 0.2000052877242094\n",
      "Iteration #8, change in parameters 0.2427155914878281\n",
      "Iteration #9, change in parameters 0.11684388139044913\n",
      "Iteration #10, change in parameters 0.04617731271778132\n",
      "Iteration #11, change in parameters 0.034542233162196075\n",
      "Iteration #12, change in parameters 0.032595917955266555\n",
      "Iteration #13, change in parameters 0.02838571753434172\n",
      "Iteration #14, change in parameters 0.02415423762683345\n",
      "Iteration #15, change in parameters 0.020612008228007808\n",
      "Iteration #16, change in parameters 0.01725856796009875\n",
      "Iteration #17, change in parameters 0.013981261718961215\n",
      "Iteration #18, change in parameters 0.011074088425102902\n",
      "Iteration #19, change in parameters 0.008652572774365114\n",
      "Finished in 19 iterations\n"
     ]
    }
   ],
   "source": [
    "bg_gmm = GaussianMixtureModel()\n",
    "bg_gmm.fit(train_bg_data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Foreground model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #1, change in parameters 86.0208009056661\n",
      "Iteration #2, change in parameters 0.043352854199836866\n",
      "Iteration #3, change in parameters 0.16036789370503013\n",
      "Iteration #4, change in parameters 0.4939995044371115\n",
      "Iteration #5, change in parameters 0.6365985697098365\n",
      "Iteration #6, change in parameters 0.5564030845937946\n",
      "Iteration #7, change in parameters 0.48418867364986345\n",
      "Iteration #8, change in parameters 0.3034050554597453\n",
      "Iteration #9, change in parameters 0.22613222860844168\n",
      "Iteration #10, change in parameters 0.2051827429699498\n",
      "Iteration #11, change in parameters 0.1994062856831696\n",
      "Iteration #12, change in parameters 0.18117950306763325\n",
      "Iteration #13, change in parameters 0.17777185036319212\n",
      "Iteration #14, change in parameters 0.17838343799748202\n",
      "Iteration #15, change in parameters 0.10377768012748415\n",
      "Iteration #16, change in parameters 0.05387292797277259\n",
      "Iteration #17, change in parameters 0.04398445768443381\n",
      "Iteration #18, change in parameters 0.02949260084435999\n",
      "Iteration #19, change in parameters 0.019170572883254154\n",
      "Iteration #20, change in parameters 0.013794137030882517\n",
      "Iteration #21, change in parameters 0.011214233422037367\n",
      "Iteration #22, change in parameters 0.010750346157035339\n",
      "Iteration #23, change in parameters 0.012651847689744753\n",
      "Iteration #24, change in parameters 0.017930967466078074\n",
      "Iteration #25, change in parameters 0.029341122192315952\n",
      "Iteration #26, change in parameters 0.040005614178797846\n",
      "Iteration #27, change in parameters 0.032174302109694325\n",
      "Iteration #28, change in parameters 0.023361374440235682\n",
      "Iteration #29, change in parameters 0.017198686497483545\n",
      "Iteration #30, change in parameters 0.01312150730684851\n",
      "Iteration #31, change in parameters 0.010300186195013328\n",
      "Iteration #32, change in parameters 0.008255358106218538\n",
      "Finished in 32 iterations\n"
     ]
    }
   ],
   "source": [
    "fg_gmm = GaussianMixtureModel()\n",
    "fg_gmm.fit(train_fg_data.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Posterior probabilities** <br>\n",
    "\n",
    "$P(class=fg\\  | \\ \\vec{x}) =  \\frac{\\lambda Norm_\\vec{x} [\\vec{\\mu}_{fg} , \\Sigma_{fg}]}\n",
    "                                   {\\lambda Norm_\\vec{x} [\\vec{\\mu}_{fg} , \\Sigma_{fg}] \\ + \\ (1 - \\lambda)\n",
    "                                   Norm_\\vec{x} [\\vec{\\mu}_{bg} , \\Sigma_{bg}]   }        $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_foreground(X, bg_gmm, fg_gmm, lmbda):\n",
    "    fg_dist = fg_gmm.predict(X)\n",
    "    bg_dist = bg_gmm.predict(X)\n",
    "    y = (lmbda * fg_dist) / ((lmbda * fg_dist) + ((1 - lmbda) * bg_dist))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = predict_foreground(validate_data[0].T, bg_gmm, fg_gmm, lmbda=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_0 = y_0.astype(int).reshape(SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(y_0, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6-Fold Cross-Validation\n",
    "Since the data set is limited, cross-validation might be used to evaluate the skill of the GMM for the unseen test data. A 6-Fold method is used, which splits the data into 6 groups of 8 puzzle pieces each. The method is popular since it results in a less biased estimate of the model's skill than other methods. The procedure used follows:\n",
    "- Shuffle the test set data randomly\n",
    "- Split the data set randomly into the 6 groups (with 8 image data each.)\n",
    "- Then for each of these groups\n",
    " - Assign the group as a test set\n",
    " - Use all the remaining groups as the training set\n",
    " - Fit the model on the training set and evaluate it using the test set\n",
    " - Compute the score and discard the model\n",
    "- Model performance is then the mean value of all scores\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
