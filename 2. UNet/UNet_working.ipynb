{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, concatenate, Dense, Softmax\n",
    "from IPython.display import SVG\n",
    "from tensorflow.keras.utils import model_to_dot\n",
    "from natsort import natsorted\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import json\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "from cachier import cachier\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from skimage import img_as_float32\n",
    "import cv2 as cv2\n",
    "from math import pi, e, sqrt, cos, sin\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ztqdm = partial(tqdm, position=0, leave=True)\n",
    "cachier = partial(cachier, pickle_reload=False, cache_dir='data/cache')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Images: 100%|██████████████████████████████████████████████████████████████████| 48/48 [00:02<00:00, 20.10it/s]\n",
      "Loading Masks: 100%|██████████████████████████████████████████████████████████████████| 48/48 [00:00<00:00, 127.33it/s]\n"
     ]
    }
   ],
   "source": [
    "SIZE = (768, 1024)\n",
    "NEW_SIZE = (256, 192)\n",
    "DATA_PATH_PAIRS = list(zip(\n",
    "    natsorted(glob(f'../data/images-{SIZE[1]}x{SIZE[0]}/*.png')),\n",
    "    natsorted(glob(f'../data/masks-{SIZE[1]}x{SIZE[0]}/*.png')),\n",
    "))\n",
    "DATA_IMGS = np.array(\n",
    "    [cv2.resize(img_as_float32(imageio.imread(img_path)), NEW_SIZE) for img_path, _ in tqdm(DATA_PATH_PAIRS, 'Loading Images')])\n",
    "DATA_MSKS = np.array(\n",
    "    [cv2.resize(img_as_float32(imageio.imread(msk_path)), NEW_SIZE) for _, msk_path in tqdm(DATA_PATH_PAIRS, 'Loading Masks')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../data/corners.json', mode='r') as f:\n",
    "    DATA_CORNER_NAMES, DATA_CORNERS = json.load(f)\n",
    "    DATA_CORNERS = np.array(DATA_CORNERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_data(data):\n",
    "    return np.subtract(data, np.min(data))/ np.subtract(np.max(data), np.min(data))\n",
    "\n",
    "def show_image(image,cs=False,cmap= None, title=None):\n",
    "    if cs:\n",
    "        norm_img = normalise_data(image)\n",
    "    else:\n",
    "        norm_img = image\n",
    "    plt.title(title)\n",
    "    plt.imshow(norm_img, cmap= cmap)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataUtils(object):\n",
    "    \n",
    "    def train_test_split(self, images, masks, corners, train_size=0.7, validation_size=0.15, test_size=0.15):\n",
    "        all_indices = np.arange(len(images))\n",
    "        np.random.shuffle(all_indices)\n",
    "        train_indices = all_indices[0: int(len(images)* train_size) + 1]\n",
    "        validate_inidices = all_indices[len(train_indices): len(train_indices) + int(len(images)* validation_size)]\n",
    "        test_indices = all_indices[len(train_indices)+len(validate_inidices): len(train_indices)+len(validate_inidices)+ int(len(images)* test_size)]\n",
    "\n",
    "        train_images = np.array([images[i] for i in train_indices])\n",
    "        train_masks = np.array([masks[i] for i in train_indices])\n",
    "        train_corners = np.array([corners[i] for i in train_indices])\n",
    "        \n",
    "        validate_images = np.array([images[i] for i in validate_inidices])\n",
    "        validate_masks = np.array([masks[i] for i in validate_inidices])\n",
    "        validate_corners = np.array([corners[i] for i in validate_inidices])\n",
    "        \n",
    "        test_images = np.array([images[i] for i in test_indices])\n",
    "        test_masks = np.array([masks[i] for i in test_indices])\n",
    "        test_corners = np.array([corners[i] for i in test_indices])\n",
    "        \n",
    "        return train_images, train_masks, train_corners,\\\n",
    "               validate_images, validate_masks, validate_corners,\\\n",
    "               test_images, test_masks, test_corners\n",
    "\n",
    "\n",
    "    def get_augmented(self, image, mask, gaus=(3, 3)):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        \n",
    "        # RGB image\n",
    "        image_list = [normalise_data(image.astype(np.float32))]\n",
    "        mask_list = [normalise_data(mask.astype(np.float32))]\n",
    "        \n",
    "        # Gaussian blurred image\n",
    "        image_ = cv2.GaussianBlur(image, gaus, 0).astype(np.float32)  \n",
    "        image_list.append(normalise_data(image_))\n",
    "        mask_list.append(normalise_data(mask.astype(np.float32)))\n",
    "        \n",
    "        # Vertical flip image \n",
    "        image_ = cv2.flip(image, 0)\n",
    "        image_list.append(normalise_data(image_))\n",
    "        mask_ = cv2.flip(mask, 0)\n",
    "        mask_list.append(normalise_data(mask_.astype(np.float32)))\n",
    "        \n",
    "        # Horizontal flip image\n",
    "        image_ = cv2.flip(image, 1)\n",
    "        image_list.append(normalise_data(image_))\n",
    "        mask_ = cv2.flip(mask, 1)\n",
    "        mask_list.append(normalise_data(mask_.astype(np.float32)))\n",
    "        \n",
    "        return image_list, mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train, validation and training split\n",
    "Split data randomly into sets of 70% training, 15% validation and 15% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_utils = ImageDataUtils()\n",
    "\n",
    "train_images, train_masks, train_corners, \\\n",
    "validate_images, validate_masks, validate_corners, \\\n",
    "test_images, test_masks, test_corners = data_utils.train_test_split(DATA_IMGS, DATA_MSKS, DATA_CORNERS,\n",
    "                                                                    train_size=0.7, validation_size=0.15, test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-process data:\n",
    "Create augmented training set (Includes pre-processing of images and masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_data = []\n",
    "augmented_masks = []\n",
    "\n",
    "for train_image, train_mask in zip(train_images, train_masks):\n",
    "    image_list, mask_list = data_utils.get_augmented(train_image, train_mask)\n",
    "    augmented_data += image_list\n",
    "    augmented_masks += mask_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process the validation and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_data = np.array([normalise_data(validate_image) for validate_image in validate_images])\n",
    "test_data = np.array([normalise_data(test_image) for test_image in test_images])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16\n",
    "\n",
    "The VGG16 model proposed was tested using the ImageNet data set, which contains over 15 million hand labeled high-resolution images, that belong to around 22-thousand categories. The model was trained for weeks, and pre-trained models are commonly available using deep learning APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 192, 256, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 192, 256, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 192, 256, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 96, 128, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 96, 128, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 96, 128, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 48, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 48, 64, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 48, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 48, 64, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 24, 32, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 24, 32, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 24, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 24, 32, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 12, 16, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 12, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 12, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 12, 16, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 6, 8, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg16_pretrained = VGG16(include_top=False, weights='imagenet', input_shape=(192,256,3))\n",
    "vgg16_pretrained.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The U-Net architecture\n",
    "U-Net was originally published as a convolutional network for biomedical image segmentation.\n",
    "The UNet model was named after its U-shape that consists of two paths, which are commonly reffered to in deep learning as the encoder- and\n",
    "decoder networks. UNet's encoder captures the context of input images, and it contains a symmetric decoder that allows to localization using a technique called upsampling (fractional convolution or transposed convolution.) \n",
    "<br><br>\n",
    "![Unet](resources/Unet.PNG)\n",
    "\n",
    "\n",
    "### VGG16-U-Net\n",
    "The idea is to use the pre-trained VGG16 model as the encoder to a U-Net model, and also to construct a symmetric decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGUnet(object):\n",
    "    \n",
    "    def __init__(self, input_shape=(192, 256, 3), num_classes=2):\n",
    "        self.input_shape = input_shape\n",
    "        self.num_classes = num_classes\n",
    "        self.model = self._build_model(self.input_shape, self.num_classes)\n",
    "        self.model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "    def summary(self):\n",
    "        print(self.model.summary())\n",
    "    \n",
    "    def _build_model(self, input_shape, num_classes):\n",
    "        \"\"\"Builds a UNet model from Keras' pretrained VGG16 model.\"\"\"\n",
    "        encoder = VGG16(include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "        \n",
    "        # Decoder block1, from Encoder output\n",
    "        encoder_output = encoder.output\n",
    "        conv_0 = Conv2D(512, 3, activation='relu', padding='same')(encoder_output)\n",
    "        conv_0_up = Conv2DTranspose(256, 3, strides=2, activation='relu', padding='same')(conv_0)\n",
    "        \n",
    "        # Decoder block 1, concat block with encoder block 5\n",
    "        concat_1 = concatenate([encoder.get_layer('block5_conv3').output, conv_0_up])\n",
    "        conv_1 = Conv2D(512, 3, activation='relu', padding='same')(concat_1)\n",
    "        conv_1_up =  Conv2DTranspose(256, 3, strides=2, activation='relu', padding='same')(conv_1)\n",
    "        \n",
    "        # Decoder block 2, concat block with encoder block 4\n",
    "        concat_2 = concatenate([encoder.get_layer('block4_conv3').output, conv_1_up])\n",
    "        conv_2 = Conv2D(512, (3, 3), activation='relu', padding='same')(concat_2)\n",
    "        conv_2_up =  Conv2DTranspose(256, 3, strides=2, activation='relu', padding='same')(conv_2)\n",
    "        \n",
    "        # Decoder block 3, concat block with encoder block 3\n",
    "        concat_3 = concatenate([encoder.get_layer('block3_conv3').output, conv_2_up])\n",
    "        conv_3 = Conv2D(256, (3, 3), activation='relu', padding='same')(concat_3)\n",
    "        conv_3_up =  Conv2DTranspose(128, 3, strides=2, activation='relu', padding='same')(conv_3)\n",
    "        \n",
    "        # Decoder block 4, concat block with encoder block 2\n",
    "        concat_4 = concatenate([encoder.get_layer('block2_conv2').output, conv_3_up])\n",
    "        conv_4 = Conv2D(128, (3, 3), activation='relu', padding='same')(concat_4)\n",
    "        conv_4_up =  Conv2DTranspose(64, 3, strides=2, activation='relu', padding='same')(conv_4)\n",
    "        \n",
    "        # Decoder block 5, concat block with encoder block 1\n",
    "        concat_5 = concatenate([encoder.get_layer('block1_conv2').output, conv_4_up])\n",
    "        conv_5 = Conv2D(32, 3, activation='relu', padding='same')(concat_5)\n",
    "        \n",
    "        outputs = Conv2D(1, 1, activation='sigmoid')(conv_5)\n",
    "        \n",
    "        return Model(inputs=encoder.layers[0].input, outputs=outputs)\n",
    "\n",
    "    def train(self, X, y):\n",
    "        return self.model.train_on_batch(X, y)[0]\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_unet = VGGUnet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 192, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 192, 256, 64) 1792        input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 192, 256, 64) 36928       block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_pool (MaxPooling2D)      (None, 96, 128, 64)  0           block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv1 (Conv2D)           (None, 96, 128, 128) 73856       block1_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block2_conv2 (Conv2D)           (None, 96, 128, 128) 147584      block2_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 48, 64, 128)  0           block2_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv1 (Conv2D)           (None, 48, 64, 256)  295168      block2_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv2 (Conv2D)           (None, 48, 64, 256)  590080      block3_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_conv3 (Conv2D)           (None, 48, 64, 256)  590080      block3_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 24, 32, 256)  0           block3_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv1 (Conv2D)           (None, 24, 32, 512)  1180160     block3_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv2 (Conv2D)           (None, 24, 32, 512)  2359808     block4_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_conv3 (Conv2D)           (None, 24, 32, 512)  2359808     block4_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 12, 16, 512)  0           block4_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv1 (Conv2D)           (None, 12, 16, 512)  2359808     block4_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv2 (Conv2D)           (None, 12, 16, 512)  2359808     block5_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_conv3 (Conv2D)           (None, 12, 16, 512)  2359808     block5_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block5_pool (MaxPooling2D)      (None, 6, 8, 512)    0           block5_conv3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 6, 8, 512)    2359808     block5_pool[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTrans (None, 12, 16, 256)  1179904     conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 12, 16, 768)  0           block5_conv3[0][0]               \n",
      "                                                                 conv2d_transpose_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 12, 16, 512)  3539456     concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTrans (None, 24, 32, 256)  1179904     conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 24, 32, 768)  0           block4_conv3[0][0]               \n",
      "                                                                 conv2d_transpose_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 24, 32, 512)  3539456     concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTrans (None, 48, 64, 256)  1179904     conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 48, 64, 512)  0           block3_conv3[0][0]               \n",
      "                                                                 conv2d_transpose_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 48, 64, 256)  1179904     concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTrans (None, 96, 128, 128) 295040      conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 96, 128, 256) 0           block2_conv2[0][0]               \n",
      "                                                                 conv2d_transpose_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 96, 128, 128) 295040      concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_transpose_9 (Conv2DTrans (None, 192, 256, 64) 73792       conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 192, 256, 128 0           block1_conv2[0][0]               \n",
      "                                                                 conv2d_transpose_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 192, 256, 32) 36896       concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 192, 256, 1)  33          conv2d_12[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 29,573,825\n",
      "Trainable params: 29,573,825\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vgg_unet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch : 1] loss : 32.38455509766936\n",
      "[Epoch : 2] loss : 17.126518251374364\n",
      "[Epoch : 3] loss : 24.090790258720517\n",
      "[Epoch : 4] loss : 14.839791372418404\n",
      "[Epoch : 5] loss : 10.236847840249538\n",
      "Total time : 113.21 mins\n"
     ]
    }
   ],
   "source": [
    "start_time = time.process_time()\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    loss = 0\n",
    "    for data, mask in zip(augmented_data, augmented_masks):\n",
    "        loss += vgg_unet.train(np.expand_dims(data, axis=0), np.expand_dims(mask, axis=0))\n",
    "    print(\"[Epoch : \" + str(epoch + 1) + \"] loss : \" + str(loss))\n",
    "elapsed_time = time.process_time() - start_time\n",
    "print(\"Total time : \" + str(round(elapsed_time/60,2)) + \" mins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_mask(image):\n",
    "    pred = vgg_unet.predict(np.expand_dims(image, axis=0))\n",
    "    pred = np.squeeze(pred)\n",
    "    pred = pred > 0.5\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = predict_mask(validate_images[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_image(pred, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
